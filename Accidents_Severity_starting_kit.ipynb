{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![](img/ministere.jpg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RAMP on detecting Accidents Severity\n",
    "\n",
    "_Chaima El Hif, Mohamed Ben Jebara, Nour El Kamel, Taher Asmi, Chaima ElMessai & Salma Ezzina_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "For each injury accident (i.e. an accident on a public roadway involving at least one vehicle and at least one victim requiring medical attention), information describing the accident is entered by the law enforcement unit (police, gendarmerie, etc.) that responded to the accident. These entries are compiled in a form called an accident analysis form. All of these forms make up the national file of traffic accidents involving injuries, known as the \"BAAC1 file\", administered by the National Interministerial Road Safety Observatory (ONISR). The databases, extracted from the BAAC file, list all traffic accidents involving injuries that occurred during a given year in metropolitan France as well as the overseas departments (Guadeloupe, Guyana, Martinique, Reunion, and Mayotte since 2012) with a simplified description. This includes information on the location of the accident, as reported, as well as information on the characteristics of the accident and its location, the vehicles involved and their victims.\n",
    "\n",
    "The files Characteristics - Places - Vehicles - Users were retrieved in csv format from the website www.data.gouv.fr. These files regroup the data related to the accidents registered during the year 2021.\n",
    "\n",
    "The database of traffic accidents with injuries of a given year is divided into 4 sections in the form of a file in csv format for each of them:\n",
    "1. The section **CHARACTERISTICS** which describes the general circumstances of the accident\n",
    "2. The **LOCATION** section which describes the main location of the accident even if it took place at an\n",
    "intersection\n",
    "3. The **VEHICLES** involved section\n",
    "4. The **USERS** involved item\n",
    "Each of the variables contained in a section must be able to be linked to the variables of the other sections. The accident identifier number (Cf. \"Num_Acc\") present in these 4 fields allows to establish a link between all the variables which describe an accident. When an accident involves several vehicles, it is also necessary to be able to link each vehicle to its occupants. This link is made by the variable \"Num_veh\".\n",
    "Most of the variables contained in the four files listed above can contain empty cells or a zero or a point. In these three cases, it is a cell that has not been filled in by the police or without any object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting started with the RAMP starting kit\n",
    "\n",
    "### Software prerequisites"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting the data\n",
    "\n",
    "The public train and test data can be downloaded by running from the root of the starting kit:\n",
    "\n",
    "    python download_data.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "\n",
    "data_train, labels_train = get_train_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "             Num_Acc  jour  mois    an   hrmn  lum dep    com  agg  int  ...  \\\n34280   202100014931    18     6  2021  21:15    2  13  13103    1    1  ...   \n97787   202100042532    26     4  2021  18:58    1  29  29217    1    2  ...   \n120191  202100052575     3     2  2021  14:36    1  75  75107    2    3  ...   \n17282   202100007488    19    11  2021  11:03    1  89  89464    2    1  ...   \n28844   202100012516    15    10  2021  08:29    2  41  41223    1    1  ...   \n\n        catu  sexe an_nais trajet secu1  secu2 secu3  locp actp  etatp  \n34280    1.0   1.0  1985.0    5.0   1.0    0.0  -1.0  -1.0   -1   -1.0  \n97787    1.0   2.0  2001.0    5.0   8.0   -1.0  -1.0   0.0    0   -1.0  \n120191   1.0   1.0  2004.0    5.0   2.0    6.0  -1.0   0.0    0   -1.0  \n17282    1.0   1.0  1934.0    3.0   1.0   -1.0  -1.0   0.0    0   -1.0  \n28844    1.0   2.0  1982.0    4.0   1.0    5.0  -1.0   0.0    0   -1.0  \n\n[5 rows x 53 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Num_Acc</th>\n      <th>jour</th>\n      <th>mois</th>\n      <th>an</th>\n      <th>hrmn</th>\n      <th>lum</th>\n      <th>dep</th>\n      <th>com</th>\n      <th>agg</th>\n      <th>int</th>\n      <th>...</th>\n      <th>catu</th>\n      <th>sexe</th>\n      <th>an_nais</th>\n      <th>trajet</th>\n      <th>secu1</th>\n      <th>secu2</th>\n      <th>secu3</th>\n      <th>locp</th>\n      <th>actp</th>\n      <th>etatp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>34280</th>\n      <td>202100014931</td>\n      <td>18</td>\n      <td>6</td>\n      <td>2021</td>\n      <td>21:15</td>\n      <td>2</td>\n      <td>13</td>\n      <td>13103</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1985.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>97787</th>\n      <td>202100042532</td>\n      <td>26</td>\n      <td>4</td>\n      <td>2021</td>\n      <td>18:58</td>\n      <td>1</td>\n      <td>29</td>\n      <td>29217</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2001.0</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>120191</th>\n      <td>202100052575</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2021</td>\n      <td>14:36</td>\n      <td>1</td>\n      <td>75</td>\n      <td>75107</td>\n      <td>2</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2004.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>17282</th>\n      <td>202100007488</td>\n      <td>19</td>\n      <td>11</td>\n      <td>2021</td>\n      <td>11:03</td>\n      <td>1</td>\n      <td>89</td>\n      <td>89464</td>\n      <td>2</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1934.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>28844</th>\n      <td>202100012516</td>\n      <td>15</td>\n      <td>10</td>\n      <td>2021</td>\n      <td>08:29</td>\n      <td>2</td>\n      <td>41</td>\n      <td>41223</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1982.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>5.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 53 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3, 3, 3, ..., 3, 3, 3])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"In this dataset we have {data_train.shape[0]} registred accidents and {data_train.shape[1]} features recovered from the different tables.\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description of the features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The challenge with this dataset is that we have numerical and categorical features. Thus, we need to work on the preprocessing of the data:\n",
    "1. Handle the missing values\n",
    "2. Transform categorical variables: Encoding\n",
    "\n",
    "In this challenge we are dealing with a multiclassification problem."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking types\n",
    "plt.figure(figsize=(20, 5))\n",
    "data_train.dtypes.value_counts().plot.pie(autopct=\"%1.1f%%\")\n",
    "plt.suptitle(\"Percentage of each type of features\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"The following are the integer features:\")\n",
    "print(data_train.select_dtypes(\"int\").columns.values)\n",
    "print(\"The following are the real features:\")\n",
    "print(data_train.select_dtypes(\"float\").columns.values)\n",
    "print(\"The following are the categorical features:\")\n",
    "print(data_train.select_dtypes(include=\"object\").columns.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this dataset we have features related to:\n",
    "1. Characteristics of the accident: Num_Acc, jour, mois, an, hrmn, lum(lighting conditions in which the accident occurred: daylight, Dusk or dawn..), dep, com, lat, long, agg(agglomération or not), int(intersection), atm(atmosphere conditions), col(type of collision), adr(adress)\n",
    "2. Location: Num_Acc, catr(road category), voie(num of roadway), V1, V2, circ(traffic regime), nbv(number of roadway)...\n",
    "3. Users: Num_Acc, Num_Veh, place, catu (user category: Driver, Passenger, Pedestrain..), sexe, an_nais(birthday year), trajet, secu, locp(Pedestrian Location), actp(Action Location), etap\n",
    "4. Vehicles: Num_Acc, Num_Veh, senc(Direction of traffic), catv(vehicle category: scooter, bike...), obs(Fixed obstacle hit), obsm (Moving obstacle hit), choc(Initial shock point), manv(Main maneuver before the accident), occutc(Number of occupants on public transit).\n",
    "\n",
    "For more detail about the features you can look at the document **\"description-de-la-liste-des-vehicules-immatricules-impliques-dans-les-accidents-corporels.pdf\"**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the integer variables for possible feature engineering\n",
    "for col in data_train.select_dtypes(exclude=\"object\"):\n",
    "    print(f\"{col :-<50} {data_train[col].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* \"an\" column has a single value since it's only the data of 2021. Can be removed because variance is null.\n",
    "* Some variables can be considered categorical features (agg, sexe...)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the integer variables for possible feature engineering\n",
    "for col in data_train.select_dtypes(\"object\"):\n",
    "    print(f\"{col :-<50} {data_train[col].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* This can give us an idea about the type of encoding we want to apply to each category column."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking Missing Data\n",
    "percentage_na = (data_train.isnull().sum() / data_train.shape[0]).sort_values(\n",
    "    ascending=False\n",
    ")\n",
    "print(\"The percentage of missing values:\")\n",
    "for column_name in percentage_na.index:\n",
    "    print(f\"{column_name} : {np.round(percentage_na.loc[column_name]*100,2)} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We see here that we have variables that have missing values. We can divide them into 2 subgroups. The first group contains on average 95% of missing values, while the second subgroup contains on average 5% of missing values.\n",
    "* For the first subgroup, we obtain 95% of missing values, so it will be good to abandon it since it will not give us any information."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Description of the target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**grav**: The target represents the severity of injury to the user. The accident victims are classified in three categories of victims plus the unharmed as following:\n",
    "0. Unharmed\n",
    "1. Killed\n",
    "2. Injured in hospital\n",
    "3. Slightly injured"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking the target\n",
    "val = pd.Series(labels_train).value_counts(normalize=True) * 100\n",
    "print(f\"The distribution of the target is as following: \\n {val.round(2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also visualize the target with bar plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualising the target distribution\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i in range(val.shape[0]):\n",
    "    plt.bar(str(val.index[i]), val.values[i], width=0.5, label=str(val.index[i]))\n",
    "\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.xlabel(\"Targets\")\n",
    "plt.legend()\n",
    "plt.suptitle(\"The distribution of the target of the train dataset\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Relation: Target & Numerical features\n",
    "In this part we will visualize our numerical features distribution and relationship with the target."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for col in data_train.select_dtypes(\"int\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    sns.boxplot(x=labels_train, y=data_train[col])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* **At first glance, we can see that some features have outliers, but we need to study it further and see if these outliers represent a specific class.**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for col in data_train.select_dtypes(\"float\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    sns.boxplot(x=labels_train, y=data_train[col])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Relation: Target & Categorical features\n",
    "\n",
    "In this part we will visualize our categorical features distribution and relationship with the target."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract categorical features that have at most 21 categories\n",
    "columns = data_train.select_dtypes(\"object\").columns[\n",
    "    data_train.select_dtypes(\"object\").nunique() <= 26\n",
    "]\n",
    "\n",
    "# visualize their relationship with the target\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    sns.countplot(x=data_train[col], hue=labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "sns.violinplot(data=data_train, x=\"actp\", y=labels_train, hue=\"sexe\")\n",
    "plt.suptitle(\"Distribution of gravity with actp and sexe\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature engineering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "*   Dealing with missing Data\n",
    "* Transformation of features\n",
    "*   Encoding Categorical features\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing Values\n",
    "\n",
    "As seen above, we can remove columns with missing values rate superior to 90% of the data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop columns with high number of missing values\n",
    "na_columns = data_train.columns[data_train.isnull().sum() / data_train.shape[0] > 0.9]\n",
    "data_train = data_train.drop(columns=na_columns)\n",
    "print(\n",
    "    f\"The new dimensions after removing the columns with missing values are: {data_train.shape}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For columns with missing values with rate less than 90%, you can choose the suitable method (removing rows with nan values, imputing the missing values...)\n",
    "\n",
    "Here we are going to remove the rows with missing values. But, you are free to try something else. Make sure you don't have NaN values in the test dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop raws\n",
    "data_train = data_train.dropna()\n",
    "print(f\"The new dimensions after removing the rows are: {data_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformation of some features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Colonne hrmn (heure de la forme hh:mm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train[\"heure\"] = [int(str(i).split(\":\")[0]) for i in data_train[\"hrmn\"]]\n",
    "data_train[\"hour_sin\"] = np.sin(data_train.heure * (2.0 * np.pi / 24))\n",
    "data_train[\"minutes\"] = [int(str(i).split(\":\")[1]) for i in data_train[\"hrmn\"]]\n",
    "data_train[\"hour_cos\"] = np.cos(data_train.heure * (2.0 * np.pi / 24))\n",
    "data_train[\"minute_sin\"] = np.sin(data_train.minutes * (2.0 * np.pi / 60))\n",
    "data_train[\"minute_cos\"] = np.cos(data_train.minutes * (2.0 * np.pi / 60))\n",
    "\n",
    "del data_train[\"hrmn\"]\n",
    "del data_train[\"heure\"]\n",
    "del data_train[\"minutes\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert long (longitude) and lat (latitude) to the correct format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Converting the columns lat and long to float instead of object:\n",
    "data_train[\"lat\"] = [float(str(i).replace(\",\", \".\")) for i in data_train[\"lat\"]]\n",
    "data_train[\"long\"] = [float(str(i).replace(\",\", \".\")) for i in data_train[\"long\"]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format the larrout feature (The width of the roadway used for vehicular traffic, excluding hard shoulder, TPCs and parking spaces)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_train[\"larrout\"] = data_train[\"larrout\"].str.replace(\",\", \".\").astype(float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing useless features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We can drop the **id_vehicule** and **Num_Acc** column since it has no effect on the model.\n",
    "* We can drop the **adr** and **voie** columns since they can be replaced by the columns lat and long\n",
    "* We can drop the **an** column since we have only data of 2021."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns_to_drop = [\"id_vehicule\", \"Num_Acc\", \"adr\", \"voie\", \"an\"]\n",
    "data_train = data_train.drop(columns=columns_to_drop)\n",
    "print(f\"The new dimensions after removing the columns are: {data_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the correlation matrix and display it\n",
    "cov_matrix = data_train.corr(numeric_only=True).abs()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(cov_matrix)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We consider that two features are correlated if they have a cov more than 0.85. In this case we can drop one of them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping correlated features\n",
    "upper_tri = cov_matrix.where(np.triu(np.ones(cov_matrix.shape), k=1).astype(bool))\n",
    "columns_to_drop = [\n",
    "    column for column in upper_tri.columns if any(upper_tri[column] > 0.85)\n",
    "]\n",
    "data_train = data_train.drop(columns=columns_to_drop)\n",
    "print(f\"We dropped the following columns: {columns_to_drop}\")\n",
    "print(f\"The new dimensions after removing the columns are: {data_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the object features\n",
    "object_col = data_train.select_dtypes(\"object\").columns\n",
    "print(\"The categorical features -- number of unique values:\")\n",
    "for col in object_col:\n",
    "    print(f\"{col :-<50} {data_train[col].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We used frequency encoding because each variable has many unique values. Therefore, it will be tedious to use a one hot encoder since it will greatly increase the size of our feature space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the numerical features\n",
    "num_col = data_train.select_dtypes(exclude=\"object\").columns\n",
    "print(\"The numerical features -- number of unique values:\")\n",
    "for col in num_col:\n",
    "    print(f\"{col :-<50} {data_train[col].nunique()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We can scale features that have more than 40 unique value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_col = [col for col in num_col if data_train[col].nunique() > 40]\n",
    "print(\"The numerical features to scale:\")\n",
    "num_col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The testing data can be loaded similarly as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from problem import get_test_data\n",
    "\n",
    "data_train, labels_train = get_train_data()\n",
    "data_test, labels_test = get_test_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Workflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The submission consists of two files: `feature_preprocessing.py` which defines a `FeaturePreprocessing` class, and `classifier.py` which defines a `Classifier` class\n",
    "\n",
    "- `FeaturePreprocessing` can hold code to format features (like done above).\n",
    "- `Classifier` fits the model and predicts on (new) data, as outputted by the `FeaturePreproceesing`. The prediction should be in the form of a (n_samples, nb_classes) array with the probabilities of each class.\n",
    "---\n",
    "\n",
    "The example of feature preprocessing on the train dataset is repeated for the test dataset plus a classifier doing a RandomForest:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reminder\n",
    "print(f\"Train set: {data_train.shape}\")\n",
    "print(f\"Test set: {data_test.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class FeaturePreprocessing(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Format and remove features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.columns_to_drop = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        na_columns = X.columns[X.isnull().sum() / X.shape[0] > 0.9]\n",
    "        self.columns_to_drop.extend(na_columns)\n",
    "\n",
    "        columns_to_drop = [\"id_vehicule\", \"Num_Acc\", \"adr\", \"voie\", \"an\", \"hrmn\"]\n",
    "        self.columns_to_drop.extend(columns_to_drop)\n",
    "\n",
    "        cov_matrix = X.corr(numeric_only=True).abs()\n",
    "        upper_tri = cov_matrix.where(\n",
    "            np.triu(np.ones(cov_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "        columns_to_drop = [\n",
    "            column for column in upper_tri.columns if any(upper_tri[column] > 0.85)\n",
    "        ]\n",
    "        self.columns_to_drop.extend(columns_to_drop)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform hrmn\n",
    "        X = X.copy()\n",
    "        X[\"heure\"] = [int(str(i).split(\":\")[0]) for i in X[\"hrmn\"]]\n",
    "        X[\"hour_sin\"] = np.sin(X.heure * (2.0 * np.pi / 24))\n",
    "        X[\"minutes\"] = [int(str(i).split(\":\")[1]) for i in X[\"hrmn\"]]\n",
    "        X[\"hour_cos\"] = np.cos(X.heure * (2.0 * np.pi / 24))\n",
    "        X[\"minute_sin\"] = np.sin(X.minutes * (2.0 * np.pi / 60))\n",
    "        X[\"minute_cos\"] = np.cos(X.minutes * (2.0 * np.pi / 60))\n",
    "\n",
    "        del X[\"heure\"]\n",
    "        del X[\"minutes\"]\n",
    "\n",
    "        # Format other features\n",
    "        X[\"lat\"] = [float(str(i).replace(\",\", \".\")) for i in X[\"lat\"]]\n",
    "        X[\"long\"] = [float(str(i).replace(\",\", \".\")) for i in X[\"long\"]]\n",
    "        X[\"larrout\"] = X[\"larrout\"].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "        # Remove columns\n",
    "        X = X.drop(columns=self.columns_to_drop)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "class CountOrdinalEncoder(OrdinalEncoder):\n",
    "    \"\"\"Encode categorical features as an integer array\n",
    "    usint count information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, categories=\"auto\", dtype=np.float64):\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the OrdinalEncoder to X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to determine the categories of each feature.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.handle_unknown = \"use_encoded_value\"\n",
    "        self.unknown_value = np.nan\n",
    "        super().fit(X)\n",
    "        X_list, _, _ = self._check_X(X)\n",
    "        # now we'll reorder by counts\n",
    "        for k, cat in enumerate(self.categories_):\n",
    "            counts = []\n",
    "            for c in cat:\n",
    "                counts.append(np.sum(X_list[k] == c))\n",
    "            order = np.argsort(counts)\n",
    "            self.categories_[k] = cat[order]\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The model to submit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "def get_estimator():\n",
    "    feature_preproc = FeaturePreprocessing()\n",
    "    transformer = ColumnTransformer(\n",
    "        [\n",
    "            (\"Num_col\", StandardScaler(), num_col),\n",
    "            (\"Object_col\", CountOrdinalEncoder(), object_col),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"feature_preprocessing\", feature_preproc),\n",
    "            (\"transformer\", transformer),\n",
    "            (\"classifier\", classifier),\n",
    "        ]\n",
    "    )\n",
    "    return pipe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using thus model interactively in the notebook to fit on the training data and predict for the testing data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_estimator()\n",
    "model.fit(data_train, labels_train - 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict_proba(data_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.named_steps[\"classifier\"].classes_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The predictions are a 4D array:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(42601, 4)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.58551826e-01, 7.44454075e-04, 2.47139008e-02, 3.15989819e-01],\n       [5.43064231e-01, 9.50904139e-04, 1.44075633e-02, 4.41577301e-01],\n       [3.52280036e-01, 2.00938219e-02, 2.24812922e-01, 4.02813220e-01],\n       ...,\n       [2.56996121e-01, 3.32970801e-01, 2.80582219e-01, 1.29450859e-01],\n       [9.99668410e-01, 1.34739649e-05, 9.58317385e-05, 2.22284746e-04],\n       [1.47350673e-02, 5.69775871e-03, 2.06477958e-01, 7.73089216e-01]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7286408426478223"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(labels_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.78     18070\n",
      "           1       0.31      0.07      0.11      1075\n",
      "           2       0.51      0.39      0.45      6262\n",
      "           3       0.66      0.67      0.67     17194\n",
      "\n",
      "    accuracy                           0.68     42601\n",
      "   macro avg       0.56      0.49      0.50     42601\n",
      "weighted avg       0.67      0.68      0.67     42601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_test, y_pred.argmax(axis=1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since it's very important to reduce the false positive of class 1 (Killed). We can consider the weighted metrics as reference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation with Cross-Validation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metrics are actually calculated using a cross-validation approach (5-fold cross-validation):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from problem import get_cv\n",
    "\n",
    "\n",
    "def evaluation(X, y):\n",
    "    pipe = get_estimator()\n",
    "    cv = get_cv(X, y)\n",
    "    results = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=[\"neg_log_loss\"],\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        return_train_score=True,\n",
    "        n_jobs=1,\n",
    "    )\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "results = evaluation(data_train, labels_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score Log Loss: 0.600 +- 0.016\n",
      "Testing score Log Loss: 0.744 +- 0.003 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Training score Log Loss: {:.3f} +- {:.3f}\".format(\n",
    "        -np.mean(results[\"train_neg_log_loss\"]), np.std(results[\"train_neg_log_loss\"])\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Testing score Log Loss: {:.3f} +- {:.3f} \\n\".format(\n",
    "        -np.mean(results[\"test_neg_log_loss\"]), np.std(results[\"test_neg_log_loss\"])\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}